# LLM Client App â€” Privacy-First Build Plan
# Stack: React Native + Expo + NativeWind + Local Auth + Local LLM (post-eject)

## Phase 1: UI & Mocked LLM

- [ X ] Set up new Expo project:
  - `npx create-expo-app llm-client-app`
  - Install NativeWind: Tailwind + NativeWind setup
  - Install Expo Router

- [ X ] Implement Chat UI:
  - ChatList (flatlist with user/assistant message bubbles)
  - InputBar (text input + send button)
  - Typing indicator

- [ X ] Mock LLM client:
  - Create `lib/llmClient.ts` with mocked delay + static reply
  - Integrate into chat screen logic

## Phase 2: Local Auth (if needed)

- [ ] (No PIN required for now; see optional section below)

## Phase 3: Persistent Local Chat Memory

- [ ] Install AsyncStorage:
  - `npm install @react-native-async-storage/async-storage`

- [ ] Implement chat persistence:
  - Store chat messages under unique thread key
  - Load on app start

- [ ] Add message timestamp and role
- [ ] Optional: implement SQLite memory later

- [ ] Create a hook: `useChatHistory(threadId)`

## Phase 4: Eject + Native LLM Runtime

- [ ] Eject from Expo:
  - `npx expo eject`

- [ ] Set up native build environment:
  - Xcode + Android Studio
  - Ensure app builds and runs post-eject

- [ ] Integrate llama.cpp or gguf runtime:
  - Clone llama.cpp repo
  - Write React Native bridge (native module)
  - Load quantized model (e.g., Mistral, TinyLlama)

- [ ] Add logic to run local inference:
  - Pass prompt
  - Stream or return full assistant response
  - Handle context window size

- [ ] Store models in file system:
  - Use `expo-file-system` (or `react-native-fs` post-eject)

- [ ] Use worker thread (or native thread) to run model off main UI

- [ ] Replace mocked LLM client with real local inference

## Bonus (Optional after Phase 4)

- [ ] Add settings page:
  - Model selection
  - Context window size
  - Temperature slider

- [ ] Add local analytics:
  - Generate anon UUID
  - Track usage events with opt-in
  - Send to Plausible or custom endpoint

- [ ] Crash reporting (self-hosted Sentry)

## Optional: Local PIN Lock (Not Needed for Now)

- [ ] (Optional) Add PIN-based local lock:
  - Setup PIN screen (shown on first run, to create a PIN)
  - Enter PIN screen (shown on every launch, to unlock)
  - Store encrypted PIN using SecureStore
  - Validate PIN on unlock
  - Implement authentication context/provider
  - Add auth gate to app
  - Auto-lock app (background/inactivity)
  - Optional: Add PIN reset flow (with app reinstall)
